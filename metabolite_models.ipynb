{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da8e154-6ef3-4465-9462-3580af40cf2b",
   "metadata": {},
   "source": [
    "### Install all required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe34af5-1282-466d-ac62-d733fca9a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "! pip install composition_stats\n",
    "! pip install --upgrade fastsrm\n",
    "! pip install xgboost\n",
    "! pip install pandas\n",
    "! pip install boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493f50e-c31b-467b-bbdb-f25bbd410f00",
   "metadata": {},
   "source": [
    "### Import all required packages and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e9920f-d1c2-4f7b-bdfc-0c86ff725b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for data manipulation:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "#for data preprocessing:\n",
    "import composition_stats as cs #for clr function\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.stats as stats\n",
    "#for model development:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from boruta import BorutaPy\n",
    "import xgboost as xgb\n",
    "#for model performance evaluation and visualization:\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer, confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6e56c-2480-4375-bc6a-9d84cc045e13",
   "metadata": {},
   "source": [
    "### Import all required data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1468806-e0a7-45ce-b437-a14af2bf2a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the preprocessed metabolite data of the fransoza dataset:\n",
    "data_met = 'fran_met.csv'\n",
    "df_met = pd.read_csv(format(data_met))\n",
    "#Rename sample column and index it:\n",
    "df_met.rename(columns={'Unnamed: 0': 'Sample'}, inplace=True)\n",
    "df_met = df_met.set_index('Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945aed81-5356-471c-b572-a03f3aa4908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the preprocessed metadata of the fransoza dataset:\n",
    "data_meta = 'fran_metadata.csv'\n",
    "df_meta = pd.read_csv(format(data_meta))\n",
    "#Rename sample column and index it:\n",
    "df_meta= df_meta.set_index('Sample')\n",
    "df_meta = df_meta.drop(\"Unnamed: 0\", axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a57498-a9eb-41e1-a76d-a0c382bd1984",
   "metadata": {},
   "source": [
    "Set the X variable to the metabolite feature data and the y variable to the disease group status (IBD or Control):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01a3c8e5-e23b-4b51-ba27-37801a469f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_met\n",
    "y = df_meta['Disease.Group']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20029dbf-edd1-435e-98fb-2559dc99e346",
   "metadata": {},
   "source": [
    "### Preprocessing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2dd155-99ef-4e45-8980-5cf26443ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#allows the preprocessing within the loop later to be one line compared to many:\n",
    "def pre_pros(x):\n",
    "    x_col = list(x.columns)\n",
    "    x_row = list(x.index)\n",
    "    as_matrix = x.values\n",
    "    clr_matrix = cs.clr(as_matrix + 1)\n",
    "    z_matrix = stats.zscore(clr_matrix)\n",
    "    tran_df = pd.DataFrame(z_matrix)\n",
    "    tran_df = tran_df.set_axis(x_row, axis=0)\n",
    "    tran_df = tran_df.set_axis(x_col, axis=1)\n",
    "    \n",
    "    return tran_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf9870-7800-492a-90ff-749529d1f9ad",
   "metadata": {},
   "source": [
    "# Model Creation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d09896-a804-4b11-9bbe-e696b2612f23",
   "metadata": {},
   "source": [
    "#### All the models are in seperate loops due to the limit of processing power and the frequency of crashing when trying to run it all as one loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41332079-54b3-4837-a37d-cb7d9dfbc96f",
   "metadata": {},
   "source": [
    "## Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a25e78-ec18-4d74-96e3-a0c20a33d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up model:\n",
    "model_names = [\"rf\"] #allows combination into a larger loop if able\n",
    "models = {'rf': RandomForestClassifier(random_state=23)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d93f2-75b2-40cd-ab70-8a22de1531e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up parameter grid of different parameter values to try in the grid search:\n",
    "param_grids = {\n",
    "       'rf': {\n",
    "       'n_estimators' : [100, 200, 300, 400, 500],\n",
    "        'criterion' : ['gini', 'entropy'],\n",
    "        'max_depth' : [10, 20, 30, 40, 50],\n",
    "        'max_features' : [ 0.25, 0.5, 0.75, 1.0], \n",
    "        'max_samples': [0.8, 0.9, 1.0]\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467bac90-f7f1-4861-87a7-ca3dacc3384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create somewhere to store all the performance metrics:\n",
    "rf_met = {'rf':defaultdict(list)} #only run first time through as will over-ride results otherwise!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8676e6-bed7-425a-8a89-8d47f266b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model for all folds of a 10 fold cross validation:\n",
    "#define the cross fold validation functions:\n",
    "skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 23) #for outer split \n",
    "skf_small = StratifiedKFold(n_splits=5, shuffle = True, random_state = 23) #for inner split within the grid search\n",
    "scorer = make_scorer(roc_auc_score, needs_threshold=True) #to use roc score in the grid search\n",
    "#outer loop which splits the data into 10 cross validation folds\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    #define the training and testing data for the current fold:\n",
    "    x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "\n",
    "    #pre-process data (scale and normalise):\n",
    "    x_train_fold = pre_pros(x_train_fold)\n",
    "    x_test_fold = pre_pros(x_test_fold)\n",
    "    \n",
    "    #make y vector numerical values:\n",
    "    y_train_fold = y_train_fold.replace({'IBD': 1, 'Control': 0})\n",
    "    y_test_fold = y_test_fold.replace({'IBD': 1, 'Control': 0})\n",
    "    \n",
    "    \n",
    "    #perform grid search on the train data:\n",
    "    for i, MODEL in enumerate(model_names): #allows one loop of models if processing allows\n",
    "        \n",
    "        time_start = time.time()\n",
    "        \n",
    "        model = models[MODEL] #select the corresponding model\n",
    "        param_grid = param_grids[MODEL] #select the corresponding parameter grid\n",
    "        grid = GridSearchCV(estimator = model, param_grid = param_grid , n_jobs = 1, cv = skf_small, scoring = scorer)\n",
    "        grid_result = grid.fit(x_train_fold, y_train_fold)\n",
    "       \n",
    "        time_grid = time.time() - time_start\n",
    "        \n",
    "        print(f\"Grid Search took {time_grid/60:.2f} mins ({time_grid:.2f} secs)\")\n",
    "    \n",
    "        #extract best model from grid search:\n",
    "        best_model = grid_result.best_estimator_\n",
    "        best_model_params = grid_result.best_params_\n",
    "        best_idx = grid_result.best_index_\n",
    "        best_grid_model_score = grid_result.cv_results_['mean_test_score'][best_idx]\n",
    "        best_grid_model_std = grid_result.cv_results_['std_test_score'][best_idx]\n",
    "        \n",
    "        #boruta feature selection on best model:\n",
    "        use_model = RandomForestClassifier(**best_model_params, random_state = 23)\n",
    "        feat_selector = BorutaPy(best_model, n_estimators='auto', verbose=2, random_state=1)\n",
    "        x_train_fold_np  = np.asarray(x_train_fold)\n",
    "        y_train_fold_np  = np.asarray(y_train_fold)\n",
    "        feat_selector.fit(x_train_fold_np, y_train_fold_np)\n",
    "        \n",
    "        # extract selected features:\n",
    "        feat_select = feat_selector.support_\n",
    "        feat_rank = feat_selector.ranking_\n",
    "        \n",
    "        #fit new model with selected features and hyperparameters on all training data:\n",
    "        re_use_model = RandomForestClassifier(**best_model_params, random_state = 23) #new copy of best model\n",
    "        #select only required features:\n",
    "        x_train_red = x_train_fold.iloc[:, feat_select]\n",
    "        x_test_red = x_test_fold.iloc[:, feat_select]\n",
    "        re_use_model.fit(x_train_red, y_train_fold)\n",
    "    \n",
    "        #use best model with selected features on test data:\n",
    "        y_pred_lab = re_use_model.predict(x_test_red)\n",
    "        y_pred_prob = re_use_model.predict_proba(x_test_red)\n",
    "        y_true_lab = y_test_fold\n",
    "        feature_importance = re_use_model.feature_importances_\n",
    "        \n",
    "        #extract performance metrics of best model on test data:\n",
    "        recall = recall_score(y_true_lab, y_pred_lab)\n",
    "        precision = precision_score(y_true_lab, y_pred_lab)\n",
    "        f1 = f1_score(y_true_lab, y_pred_lab)\n",
    "        confusion = confusion_matrix(y_true_lab, y_pred_lab)\n",
    "        TP = confusion[1, 1]\n",
    "        TN = confusion[0, 0]\n",
    "        FP = confusion[0, 1]\n",
    "        FN = confusion[1, 0]\n",
    "        specificity = TN / (TN + FP)\n",
    "        sensitivity = TP / (TP + FN)\n",
    "\n",
    "        fpr_num, tpr_num, _ = roc_curve(y_test_fold, y_pred_prob[:,1])\n",
    "        auc_score = auc(fpr_num, tpr_num)\n",
    "        \n",
    "        \n",
    "        #store metrics:\n",
    "        rf_met[MODEL]['best_model'].append(best_model)\n",
    "        rf_met[MODEL]['best_params'].append(best_model_params)\n",
    "        rf_met[MODEL]['feat_select'].append(feat_select)\n",
    "        rf_met[MODEL]['feat_rank'].append(feat_rank)\n",
    "        rf_met[MODEL]['feat_import'].append(feature_importance)\n",
    "        rf_met[MODEL]['true_lab'].append(y_true_lab)\n",
    "        rf_met[MODEL]['pred_lab'].append(y_pred_lab)\n",
    "        rf_met[MODEL]['pred_prob'].append(y_pred_prob)\n",
    "        rf_met[MODEL]['recall'].append(recall)\n",
    "        rf_met[MODEL]['precision'].append(precision)\n",
    "        rf_met[MODEL]['f1'].append(f1)\n",
    "        rf_met[MODEL]['specificity'].append(specificity)\n",
    "        rf_met[MODEL]['sensitivity'].append(sensitivity)\n",
    "        rf_met[MODEL]['best_mod_auc'].append(best_grid_model_score)\n",
    "        rf_met[MODEL]['best_mod_std'].append(best_grid_model_std)\n",
    "        rf_met[MODEL]['test_auc'].append(auc_score)\n",
    "        \n",
    "        print(rf_met[MODEL]) #allows you to idenitfy when a loop is complete and view that loops metrics \n",
    "        \n",
    "        %store rf_met #allows you to extract data later without having to re-run the whole loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b7639-e2b0-49ca-b6dd-f97c400fcfe2",
   "metadata": {},
   "source": [
    "## LASSO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77cb788-14c7-43a8-bfca-60301f2c2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up model:\n",
    "model_names = [\"lasso\"]  #allows combination into a larger loop if able\n",
    "models = {'lasso': Lasso(random_state = 23) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a0fc2-1452-4cb8-8421-31137dd2ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up parameter grid of different parameter values to try in the grid search:\n",
    "param_grids = {\n",
    "       'lasso': {\n",
    "       'alpha' : list(np.logspace(-4,2,7))   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e617b-0417-4fed-a31a-e69dd7462b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create somewhere to store all the performance metrics:\n",
    "lasso_met = {'lasso':defaultdict(list)} #only run first time through as will over-ride results otherwise!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0261e4e-25c3-4af0-a422-56f57e5ddd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model for all folds of a 10 fold cross validation:\n",
    "#define the cross fold validation functions:\n",
    "skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 23) #for outer split \n",
    "skf_small = StratifiedKFold(n_splits=5, shuffle = True, random_state = 23) #for inner split within the grid search\n",
    "scorer = make_scorer(roc_auc_score, needs_threshold=True) #to use roc score in the grid search\n",
    "#outer loop which splits the data into 10 cross validation folds:\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    #define the training and testing data for the current fold:\n",
    "    x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "\n",
    "    #pre-process data (scale and normalise):\n",
    "    x_train_fold = pre_pros(x_train_fold)\n",
    "    x_test_fold = pre_pros(x_test_fold)\n",
    "    \n",
    "    #make y vector numerical:\n",
    "    y_train_fold = y_train_fold.replace({'IBD': 1, 'Control': 0})\n",
    "    y_test_fold = y_test_fold.replace({'IBD': 1, 'Control': 0})\n",
    "    \n",
    "    \n",
    "    #perform grid search on the train data:\n",
    "    for i, MODEL in enumerate(model_names): #allows one loop for all models if processing allows\n",
    "        \n",
    "        time_start = time.time()\n",
    "        \n",
    "        model = models[MODEL] #select corresponding model\n",
    "        param_grid = param_grids[MODEL] #select corresponding parameter grid\n",
    "        grid = GridSearchCV(estimator = model, param_grid = param_grid , n_jobs = 1, cv = skf_small, scoring = scorer)\n",
    "        grid_result = grid.fit(x_train_fold, y_train_fold)\n",
    "       \n",
    "        time_grid = time.time() - time_start\n",
    "        \n",
    "        print(f\"Grid Search took {time_grid/60:.2f} mins ({time_grid:.2f} secs)\")\n",
    "    \n",
    "        #extract best model from the grid search:\n",
    "        best_model = grid_result.best_estimator_\n",
    "        best_model_params = grid_result.best_params_\n",
    "        best_idx = grid_result.best_index_\n",
    "        best_grid_model_score = grid_result.cv_results_['mean_test_score'][best_idx]\n",
    "        best_grid_model_std = grid_result.cv_results_['std_test_score'][best_idx]\n",
    "    \n",
    "        #extract important features as selected by the built in LASSO model:\n",
    "        feat_select = (best_model.coef_ != 0)\n",
    "    \n",
    "        #use best model parameters and features on test data:\n",
    "        y_pred_prob = best_model.predict(x_test_fold)\n",
    "        y_pred_lab = np.where(y_pred_prob >= 0.5, 1, 0) #defined as IBD if probability is over 0.5\n",
    "        y_true_lab = y_test_fold\n",
    "        \n",
    "        #extract performance metrics of best model on test data:\n",
    "        recall = recall_score(y_true_lab, y_pred_lab)\n",
    "        precision = precision_score(y_true_lab, y_pred_lab)\n",
    "        f1 = f1_score(y_true_lab, y_pred_lab)\n",
    "        confusion = confusion_matrix(y_true_lab, y_pred_lab)\n",
    "        TP = confusion[1, 1]\n",
    "        TN = confusion[0, 0]\n",
    "        FP = confusion[0, 1]\n",
    "        FN = confusion[1, 0]\n",
    "        specificity = TN / (TN + FP)\n",
    "        sensitivity = TP / (TP + FN)\n",
    "\n",
    "        fpr_num, tpr_num, _ = roc_curve(y_test_fold, y_pred_prob)\n",
    "        auc_score = auc(fpr_num, tpr_num)\n",
    "        \n",
    "        \n",
    "        #store metrics:\n",
    "        lasso_met[MODEL]['best_model'].append(best_model)\n",
    "        lasso_met[MODEL]['best_params'].append(best_model_params)\n",
    "        lasso_met[MODEL]['true_lab'].append(y_true_lab)\n",
    "        lasso_met[MODEL]['pred_lab'].append(y_pred_lab)\n",
    "        lasso_met[MODEL]['pred_prob'].append(y_pred_prob)\n",
    "        lasso_met[MODEL]['recall'].append(recall)\n",
    "        lasso_met[MODEL]['precision'].append(precision)\n",
    "        lasso_met[MODEL]['feat_select'].append(feat_select)\n",
    "        lasso_met[MODEL]['f1'].append(f1)\n",
    "        lasso_met[MODEL]['specificity'].append(specificity)\n",
    "        lasso_met[MODEL]['sensitivity'].append(sensitivity)\n",
    "        lasso_met[MODEL]['best_mod_auc'].append(best_grid_model_score)\n",
    "        lasso_met[MODEL]['best_mod_std'].append(best_grid_model_std)\n",
    "        lasso_met[MODEL]['test_auc'].append(auc_score)\n",
    "        \n",
    "        print(lasso_met[MODEL])  #allows you to idenitfy when a loop is complete and view that loops metrics \n",
    "        \n",
    "        %store lasso_met #allows you to extract data later without having to re-run the whole loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc91d3-e95b-4883-8670-a04907c65745",
   "metadata": {},
   "source": [
    "## XGBOOST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b192883-0428-4cce-a617-ec369750b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up model:\n",
    "model_names = [\"xgboost\"] #allows combination into a larger loop if able\n",
    "models = {'xgboost': xgb.XGBClassifier(objective = 'binary:logistic', seed= 23)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fe0b33-c65e-4cc2-ad69-9700ac545ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up parameter grid of different parameter values to try in the grid search:\n",
    "param_grids = {\n",
    "     'xgboost': {\n",
    "        'n_estimators': [100, 200, 300, 400, 500],\n",
    "        'max_depth': [ 10, 20, 30, 40, 50],\n",
    "        'learning_rate': [0.1, 0.01, 0.05],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "       'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "   }\n",
    "}\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c9e5b-afcf-4fab-b8b3-032669653eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create somewhere to store all the performance metrics:\n",
    "xg_met = {'xgboost' : defaultdict(list)} #only run first time through as will over-ride results otherwise!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8b90e-11c9-4c98-a7c6-7efc1fc8ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model for all folds of a 10 fold cross validation:\n",
    "#define the cross fold validation functions:\n",
    "skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 23) #for outer split \n",
    "skf_small = StratifiedKFold(n_splits=5, shuffle = True, random_state = 23) #for inner split within the grid search\n",
    "scorer = make_scorer(roc_auc_score, needs_threshold=True) #to use roc score in the grid search\n",
    "#outer loop which splits the data into 10 cross validation folds\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    #define the training and testing data for the current fold:\n",
    "    x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "\n",
    "    #pre-process data (scale and normalise):\n",
    "    x_train_fold = pre_pros(x_train_fold)\n",
    "    x_test_fold = pre_pros(x_test_fold)\n",
    "    \n",
    "    #required numerical y value for xg:\n",
    "    y_train_fold = y_train_fold.replace({'IBD': 1, 'Control': 0})\n",
    "    y_test_fold = y_test_fold.replace({'IBD': 1, 'Control': 0})\n",
    "    \n",
    "    #perform grid search on the train data:\n",
    "    for i, MODEL in enumerate(model_names): #allows one loop of models if processing allows\n",
    "        \n",
    "        time_start = time.time()\n",
    "        \n",
    "        model = models[MODEL] #select the corresponding model\n",
    "        param_grid = param_grids[MODEL] #select the corresponding parameter grid \n",
    "        grid = GridSearchCV(estimator = model, param_grid = param_grid , n_jobs = 1, cv = skf_small, scoring = scorer)\n",
    "        grid_result = grid.fit(x_train_fold, y_train_fold)\n",
    "       \n",
    "        time_grid = time.time() - time_start\n",
    "        \n",
    "        print(f\"Grid Search took {time_grid/60:.2f} mins ({time_grid:.2f} secs)\")\n",
    "    \n",
    "        #extract best model from the grid search:\n",
    "        best_model = grid_result.best_estimator_\n",
    "        best_model_params = grid_result.best_params_\n",
    "        best_idx = grid_result.best_index_\n",
    "        best_grid_model_score = grid_result.cv_results_['mean_test_score'][best_idx]\n",
    "        best_grid_model_std = grid_result.cv_results_['std_test_score'][best_idx]\n",
    "        \n",
    "        #boruta feature selection using the best model:\n",
    "        use_model = xgb.XGBClassifier(**best_model_params, objective = 'binary:logistic', seed= 23)\n",
    "        feat_selector = BorutaPy(best_model, n_estimators='auto', verbose=2, random_state=1)\n",
    "        x_train_fold_np  = np.asarray(x_train_fold)\n",
    "        y_train_fold_np  = np.asarray(y_train_fold)\n",
    "        feat_selector.fit(x_train_fold_np, y_train_fold_np)\n",
    "        \n",
    "        # extract selected features:\n",
    "        feat_select = feat_selector.support_\n",
    "        feat_rank = feat_selector.ranking_\n",
    "        \n",
    "        #fit new model with selected features and hyperparameters on all training data:\n",
    "        re_use_model = xgb.XGBClassifier(**best_model_params, objective = 'binary:logistic', seed= 23)\n",
    "        #select only required features:\n",
    "        x_train_red = x_train_fold.iloc[:, feat_select]\n",
    "        x_test_red = x_test_fold.iloc[:, feat_select]\n",
    "        re_best_model.fit(x_train_red, y_train_fold)\n",
    "    \n",
    "        #use best model with selected features on test data:\n",
    "        y_pred_lab = re_best_model.predict(x_test_red)\n",
    "        y_pred_prob = re_best_model.predict_proba(x_test_red)\n",
    "        y_true_lab = y_test_fold\n",
    "        feature_importance = re_best_model.feature_importances_\n",
    "        \n",
    "        #extract performance metrics of best model on test data:\n",
    "        recall = recall_score(y_true_lab, y_pred_lab)\n",
    "        precision = precision_score(y_true_lab, y_pred_lab)\n",
    "        f1 = f1_score(y_true_lab, y_pred_lab)\n",
    "        confusion = confusion_matrix(y_true_lab, y_pred_lab)\n",
    "        TP = confusion[1, 1]\n",
    "        TN = confusion[0, 0]\n",
    "        FP = confusion[0, 1]\n",
    "        FN = confusion[1, 0]\n",
    "        specificity = TN / (TN + FP)\n",
    "        sensitivity = TP / (TP + FN)\n",
    "\n",
    "        fpr_num, tpr_num, _ = roc_curve(y_test_fold, y_pred_prob[:,1])\n",
    "        auc_score = auc(fpr_num, tpr_num)\n",
    "        \n",
    "        \n",
    "        #store metrics:\n",
    "        xg_met[MODEL]['best_model'].append(best_model)\n",
    "        xg_met[MODEL]['best_params'].append(best_model_params)\n",
    "        xg_met[MODEL]['feat_select'].append(feat_select)\n",
    "        xg_met[MODEL]['feat_rank'].append(feat_rank)\n",
    "        xg_met[MODEL]['feat_import'].append(feature_importance)\n",
    "        xg_met[MODEL]['true_lab'].append(y_true_lab)\n",
    "        xg_met[MODEL]['pred_lab'].append(y_pred_lab)\n",
    "        xg_met[MODEL]['pred_prob'].append(y_pred_prob)\n",
    "        xg_met[MODEL]['recall'].append(recall)\n",
    "        xg_met[MODEL]['precision'].append(precision)\n",
    "        xg_met[MODEL]['f1'].append(f1)\n",
    "        xg_met[MODEL]['specificity'].append(specificity)\n",
    "        xg_met[MODEL]['sensitivity'].append(sensitivity)\n",
    "        xg_met[MODEL]['best_mod_auc'].append(best_grid_model_score)\n",
    "        xg_met[MODEL]['best_mod_std'].append(best_grid_model_std)\n",
    "        xg_met[MODEL]['test_auc'].append(auc_score)\n",
    "        \n",
    "        print(xg_met[MODEL]) #allows you to idenitfy when a loop is complete and view that loops metrics \n",
    "        \n",
    "        %store xg_met #allows you to extract data later without having to re-run the whole loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5d32d-8592-4f96-89c7-ae5bbfc01f63",
   "metadata": {},
   "source": [
    "# Extract feature names and visualize model performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba6aad-1889-4178-a99c-e61052921cef",
   "metadata": {},
   "source": [
    "### Extract model performance metrics from store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4105c7-89a0-42e9-b1d2-5e699758f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "store -r rf_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb889e6-54fc-47df-bb90-b67aedb0bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "store -r xg_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eaefc3-7386-4e37-9b19-469aa7026aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "store -r lasso_met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c52df6-1db6-42c6-84d6-2802dd136621",
   "metadata": {},
   "source": [
    "### Extract feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d4f18-6046-4b34-bf76-657612525fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_features = []\n",
    "#append features names to a list from the data for all folds:\n",
    "for fold in range(0,10):\n",
    "    imp_feat = X.iloc[:,(lasso_met['lasso']['feat_select'][fold] == True)]\n",
    "    features = imp_feat.columns\n",
    "    \n",
    "    lasso_features.append(list(features))\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c93c9-aacf-4f55-82d4-af688b0f7c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all features from all folds into one big list:\n",
    "lasso_features = list(chain(*lasso_features))\n",
    "#extract the names of all the unique features:\n",
    "unique_lasso = np.unique(lasso_features)\n",
    "#find the number of unique features:\n",
    "len(unique_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949728d9-469b-4852-9731-d12514bd9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_features = []\n",
    "#append features names to a list from the data for all folds:\n",
    "for fold in range(0,10):\n",
    "    imp_feat = X.iloc[:,(rf_met['rf']['feat_select'][fold] == True)]\n",
    "    features = imp_feat.columns\n",
    "    \n",
    "    rf_features.append(list(features))\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73161f98-e58f-4815-830d-86deef881c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all features from all folds into one big list:\n",
    "rf_features = list(chain(*rf_features))\n",
    "#extract the names of all the unique features:\n",
    "unique_rf = np.unique(rf_features)\n",
    "#find the number of unique features:\n",
    "len(unique_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dfeac7-017b-4d17-acef-407b29928d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_features = []\n",
    "#append features names to a list from the data for all folds:\n",
    "for fold in range(0,10):\n",
    "    imp_feat = X.iloc[:,(xg_met['xgboost']['feat_select'][fold] == True)]\n",
    "    features = imp_feat.columns\n",
    "    \n",
    "    xg_features.append(list(features))\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ab293c-43a1-4b70-a568-f4dd5ce369c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all features from all folds into one big list:\n",
    "xg_features = list(chain(*xg_features))\n",
    "#extract the names of all the unique features:\n",
    "unique_xg = np.unique(xg_features)\n",
    "#find the number of unique features:\n",
    "len(unique_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0452dc64-c583-4d13-8ad7-8b509ced2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain the number of features that are identified by more than one model:\n",
    "len(set(unique_xg) & set(unique_rf) & set(unique_lasso))\n",
    "len((set(unique_rf) & set(unique_xg)))\n",
    "len((set(unique_xg) & set(unique_lasso)))\n",
    "len((set(unique_rf) & set(unique_lasso)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f57ad-9244-4fb4-bd0a-e8d99bade96b",
   "metadata": {},
   "source": [
    "### Extract the average and sd of the performance metrics across the 10 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a543cda-a52b-4ca8-b6bd-3586b58e7b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest averages:\n",
    "print(np.mean(rf_met['rf']['recall']))\n",
    "print(np.mean(rf_met['rf']['precision']))\n",
    "print(np.mean(rf_met['rf']['f1']))\n",
    "print(np.mean(rf_met['rf']['specificity']))\n",
    "print(np.mean(rf_met['rf']['sensitivity']))\n",
    "print(np.mean(rf_met['rf']['test_auc']))\n",
    "#random forest sds:\n",
    "print(np.std(rf_met['rf']['recall']))\n",
    "print(np.std(rf_met['rf']['precision']))\n",
    "print(np.std(rf_met['rf']['f1']))\n",
    "print(np.std(rf_met['rf']['specificity']))\n",
    "print(np.std(rf_met['rf']['sensitivity']))\n",
    "print(np.std(rf_met['rf']['test_auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf8aa6-5beb-49aa-87c6-b3cea170f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso averages:\n",
    "print(np.mean(lasso_met['lasso']['recall']))\n",
    "print(np.mean(lasso_met['lasso']['precision']))\n",
    "print(np.mean(lasso_met['lasso']['f1']))\n",
    "print(np.mean(lasso_met['lasso']['specificity']))\n",
    "print(np.mean(lasso_met['lasso']['sensitivity']))\n",
    "print(np.mean(lasso_met['lasso']['test_auc']))\n",
    "#lasso sds:\n",
    "print(np.std(lasso_met['lasso']['recall']))\n",
    "print(np.std(lasso_met['lasso']['precision']))\n",
    "print(np.std(lasso_met['lasso']['f1']))\n",
    "print(np.std(lasso_met['lasso']['specificity']))\n",
    "print(np.std(lasso_met['lasso']['sensitivity']))\n",
    "print(np.std(lasso_met['lasso']['test_auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f98727-0fe1-4dd1-a31e-8465a94780c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost averages:\n",
    "print(np.mean(xg_met['xgboost']['recall']))\n",
    "print(np.mean(xg_met['xgboost']['precision']))\n",
    "print(np.mean(xg_met['xgboost']['f1']))\n",
    "print(np.mean(xg_met['xgboost']['specificity']))\n",
    "print(np.mean(xg_met['xgboost']['sensitivity']))\n",
    "print(np.mean(xg_met['xgboost']['test_auc']))\n",
    "#xgboost sds:\n",
    "print(np.std(xg_met['xgboost']['recall']))\n",
    "print(np.std(xg_met['xgboost']['precision']))\n",
    "print(np.std(xg_met['xgboost']['f1']))\n",
    "print(np.std(xg_met['xgboost']['specificity']))\n",
    "print(np.std(xg_met['xgboost']['sensitivity']))\n",
    "print(np.std(xg_met['xgboost']['test_auc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c62c79-3fae-40dc-bc7a-4e0858941951",
   "metadata": {},
   "source": [
    "### ROC curve of all the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a06544-f6f6-41d5-84af-786a9a07a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists to store true positive rates and set false positive rates:\n",
    "tprs_rf = []\n",
    "base_fpr_rf = np.linspace(0, 1, 101)\n",
    "tprs_xg = []\n",
    "base_fpr_xg = np.linspace(0, 1, 101)\n",
    "tprs_lasso = []\n",
    "base_fpr_lasso = np.linspace(0, 1, 101)\n",
    "\n",
    "#set up blank figure space:\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "\n",
    "#calculate the corresponding tprs for the fprs of each fold of each model:\n",
    "for i in range(0,10):\n",
    "    fpr, tpr, _ = roc_curve(rf_met['rf']['true_lab'][i], rf_met['rf']['pred_prob'][i][:,1])\n",
    "    \n",
    "    tpr = np.interp(base_fpr_rf, fpr, tpr)\n",
    "    tpr[0] = 0.0\n",
    "    tprs_rf.append(tpr)\n",
    "\n",
    "for i in range(0,10):\n",
    "    fpr, tpr, _ = roc_curve(lasso_met['lasso']['true_lab'][i], lasso_met['lasso']['pred_prob'][i])\n",
    "    \n",
    "    tpr = np.interp(base_fpr_lasso, fpr, tpr)\n",
    "    tpr[0] = 0.0\n",
    "    tprs_lasso.append(tpr)\n",
    "\n",
    "for i in range(0,10):\n",
    "    fpr, tpr, _ = roc_curve(xg_met['xgboost']['true_lab'][i], xg_met['xgboost']['pred_prob'][i][:,1])\n",
    "    \n",
    "    tpr = np.interp(base_fpr_xg, fpr, tpr)\n",
    "    tpr[0] = 0.0\n",
    "    tprs_xg.append(tpr)\n",
    "\n",
    "#find the average and sd of the tprs for each fold of each model:\n",
    "tprs_rf = np.array(tprs_rf)\n",
    "mean_tprs_rf = tprs_rf.mean(axis=0)\n",
    "std_rf = tprs_rf.std(axis=0)\n",
    "\n",
    "tprs_lasso = np.array(tprs_lasso)\n",
    "mean_tprs_lasso = tprs_lasso.mean(axis=0)\n",
    "std_lasso = tprs_lasso.std(axis=0)\n",
    "\n",
    "tprs_xg = np.array(tprs_xg)\n",
    "mean_tprs_xg = tprs_xg.mean(axis=0)\n",
    "std_xg = tprs_xg.std(axis=0)\n",
    "\n",
    "#find the confidence interval for the average tprs of each model:\n",
    "tprs_upper_rf = np.minimum(mean_tprs_rf + std_rf, 1)\n",
    "tprs_lower_rf = mean_tprs_rf - std_rf\n",
    "tprs_upper_xg = np.minimum(mean_tprs_xg + std_xg, 1)\n",
    "tprs_lower_xg = mean_tprs_xg - std_xg\n",
    "tprs_upper_lasso = np.minimum(mean_tprs_lasso + std_lasso, 1)\n",
    "tprs_lower_lasso = mean_tprs_lasso - std_lasso\n",
    "\n",
    "#calculate the area under the roc for each of the average models performance:\n",
    "auc_rf = auc(base_fpr_rf, mean_tprs_rf)\n",
    "auc_xg = auc(base_fpr_xg, mean_tprs_xg)\n",
    "auc_lasso = auc(base_fpr_lasso, mean_tprs_lasso)\n",
    "\n",
    "#plot the average roc for each model with the confidence interval surrounding:\n",
    "plt.plot(base_fpr_rf, mean_tprs_rf, 'palevioletred', label=r'Mean ROC RandomForest (AUC = {} $\\pm$ {})'.format(round(auc_rf,2), 0.05))\n",
    "plt.fill_between(base_fpr_rf, tprs_lower_rf, tprs_upper_rf, color='pink', alpha=0.3)\n",
    "plt.plot(base_fpr_xg, mean_tprs_xg, 'darkcyan', label=r'Mean ROC XGBoost (AUC = {} $\\pm$ {})'.format(round(auc_xg,2), 0.07))\n",
    "plt.fill_between(base_fpr_xg, tprs_lower_xg, tprs_upper_xg, color='deepskyblue', alpha=0.3)\n",
    "plt.plot(base_fpr_lasso, mean_tprs_lasso, 'orange', label=r'Mean ROC LASSO (AUC = {} $\\pm$ {})'.format(round(auc_lasso,2), 0.04))\n",
    "plt.fill_between(base_fpr_lasso, tprs_lower_lasso, tprs_upper_lasso, color='sandybrown', alpha=0.2)\n",
    "\n",
    "#plot on the blank figure space and add title labels:\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('Receiver Operating Characteristic Metabolite Model Averages')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.savefig('mtb_avg.pdf', format = 'pdf', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 [python/3.9.6]",
   "language": "python",
   "name": "sys_python396"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
