{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77710890-3390-4fca-bcb9-52258438b904",
   "metadata": {},
   "source": [
    "### Install all required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071487ed-52a1-4023-8232-7e3a82f0e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "! pip install composition_stats\n",
    "! pip install --upgrade fastsrm\n",
    "! pip install xgboost\n",
    "! pip install pandas\n",
    "! pip install boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7459a8a8-91d4-4ef6-a05a-37496d498914",
   "metadata": {},
   "source": [
    "### Import all required packages and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb310cad-99b7-4c44-8560-e9cf8c898b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for data manipulation:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "#for data preprocessing:\n",
    "import composition_stats as cs #for clr function\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#for model development:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from boruta import BorutaPy\n",
    "import xgboost as xgb\n",
    "#for model performance evaluation and visualization:\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer, confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508efe0-3740-47e0-a2ea-7684862ca277",
   "metadata": {},
   "source": [
    "### Import all required data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7210f6-3790-4b92-b3df-094d2f7480df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import processed genus data from the fransoza dataset:\n",
    "data_genus = 'fran_genera.csv'\n",
    "df_genus = pd.read_csv(format(data_genus))\n",
    "#Rename sample column and index it:\n",
    "df_genus.rename(columns={'Unnamed: 0': 'Sample'}, inplace=True)\n",
    "df_genus= df_genus.set_index('Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d927e9d2-6232-45a7-bba6-42883a05fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import processed metadata from the fransoza dataset:\n",
    "data_meta = 'fran_metadata.csv'\n",
    "df_meta = pd.read_csv(format(data_meta))\n",
    "#Rename sample column and index it:\n",
    "df_meta= df_meta.set_index('Sample')\n",
    "df_meta = df_meta.drop(\"Unnamed: 0\", axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2adff5-28f3-4c10-8b85-eb5aca51e980",
   "metadata": {},
   "source": [
    "Set the X variable to the genus feature data and the y variable to the disease group status (IBD or Control):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ee15b-afcd-4088-942f-6a6e6a8a4ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_genus\n",
    "y = df_meta['Disease.Group']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad885f5-f260-4dc0-9487-eb066e596cd8",
   "metadata": {},
   "source": [
    "### Preprocessing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f97bf-6298-479e-9dc1-c4a03ea0fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#allows the preprocessing within the loop later to be one line compared to many:\n",
    "def pre_pros(x):\n",
    "    x_col = list(x.columns)\n",
    "    x_row = list(x.index)\n",
    "    as_matrix = x.values\n",
    "    clr_matrix = cs.clr(as_matrix + 0.00001)\n",
    "    min_max_matrix = MinMaxScaler().fit_transform(clr_matrix)\n",
    "    tran_df = pd.DataFrame(min_max_matrix)\n",
    "    tran_df = tran_df.set_axis(x_row, axis=0)\n",
    "    tran_df = tran_df.set_axis(x_col, axis=1)\n",
    "    \n",
    "    return tran_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb68221-682e-4995-a7a3-afce16aeba3a",
   "metadata": {},
   "source": [
    "# Model Creation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3f86f-5732-4527-bd9a-ddec7d33969e",
   "metadata": {},
   "source": [
    "#### All the models are in seperate loops due to the limit of processing power and the frequency of crashing when trying to run it all as one loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4028c54-a297-4ee3-9cc8-ab0e1eae127d",
   "metadata": {},
   "source": [
    "## XGBOOST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea1c89-bbbb-4455-b70f-c2a07540b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up model:\n",
    "model_names = [\"xgboost\"] #allows combination into a larger loop if able\n",
    "models = {'xgboost': xgb.XGBClassifier(objective = 'binary:logistic', seed= 23)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3558fe1-a0c6-4c80-be2c-163e25359d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up parameter grid of different parameter values to try in the grid search:\n",
    "param_grids = {\n",
    "     'xgboost': {\n",
    "        'n_estimators': [100, 200, 300, 400, 500],\n",
    "        'max_depth': [ 10, 20, 30, 40, 50],\n",
    "        'learning_rate': [0.1, 0.01, 0.05],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "       'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "   }\n",
    "}\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1bd0e-183a-4bd4-bf6c-703bd2fbc015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create somewhere to store all the performance metrics:\n",
    "xg_model = {'xgboost' : defaultdict(list)} #only run first time through as will over-ride results otherwise!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf4e70-f4ef-4661-a35f-97f87bd2ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model for all folds of a 10 fold cross validation:\n",
    "#define the cross fold validation functions:\n",
    "skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 23) #for outer split \n",
    "skf_small = StratifiedKFold(n_splits=5, shuffle = True, random_state = 23) #for inner split within the grid search\n",
    "scorer = make_scorer(roc_auc_score, needs_threshold=True) #to use roc score in the grid search\n",
    "#outer loop which splits the data into 10 cross validation folds\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    #define the training and testing data for the current fold:\n",
    "    x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "\n",
    "    #pre-process data (scale and normalise):\n",
    "    x_train_fold = pre_pros(x_train_fold)\n",
    "    x_test_fold = pre_pros(x_test_fold)\n",
    "    \n",
    "    #required numerical y value for xg:\n",
    "    y_train_fold = y_train_fold.replace({'IBD': 1, 'Control': 0})\n",
    "    y_test_fold = y_test_fold.replace({'IBD': 1, 'Control': 0})\n",
    "    \n",
    "    #perform grid search on the train data:\n",
    "    for i, MODEL in enumerate(model_names): #allows one loop of models if processing allows\n",
    "        \n",
    "        time_start = time.time()\n",
    "        \n",
    "        model = models[MODEL] #select the corresponding model\n",
    "        param_grid = param_grids[MODEL] #select the corresponding parameter grid \n",
    "        grid = GridSearchCV(estimator = model, param_grid = param_grid , n_jobs = 1, cv = skf_small, scoring = scorer)\n",
    "        grid_result = grid.fit(x_train_fold, y_train_fold)\n",
    "       \n",
    "        time_grid = time.time() - time_start\n",
    "        \n",
    "        print(f\"Grid Search took {time_grid/60:.2f} mins ({time_grid:.2f} secs)\")\n",
    "    \n",
    "        #extract best model from the grid search:\n",
    "        best_model = grid_result.best_estimator_\n",
    "        best_model_params = grid_result.best_params_\n",
    "        best_idx = grid_result.best_index_\n",
    "        best_grid_model_score = grid_result.cv_results_['mean_test_score'][best_idx]\n",
    "        best_grid_model_std = grid_result.cv_results_['std_test_score'][best_idx]\n",
    "        \n",
    "        #boruta feature selection using the best model:\n",
    "        feat_selector = BorutaPy(best_model, n_estimators='auto', verbose=2, random_state=1)\n",
    "        x_train_fold_np  = np.asarray(x_train_fold)\n",
    "        y_train_fold_np  = np.asarray(y_train_fold)\n",
    "        feat_selector.fit(x_train_fold_np, y_train_fold_np)\n",
    "        \n",
    "        # extract selected features:\n",
    "        feat_select = feat_selector.support_\n",
    "        feat_rank = feat_selector.ranking_\n",
    "        \n",
    "        #fit new model with selected features and hyperparameters on all training data:\n",
    "        re_best_model = grid_result.best_estimator_ #new copy of best model\n",
    "        #select only required features:\n",
    "        x_train_red = x_train_fold.iloc[:, feat_select]\n",
    "        x_test_red = x_test_fold.iloc[:, feat_select]\n",
    "        re_best_model.fit(x_train_red, y_train_fold)\n",
    "    \n",
    "        #use best model with selected features on test data:\n",
    "        y_pred_lab = re_best_model.predict(x_test_red)\n",
    "        y_pred_prob = re_best_model.predict_proba(x_test_red)\n",
    "        y_true_lab = y_test_fold\n",
    "        feature_importance = re_best_model.feature_importances_\n",
    "        \n",
    "        #extract performance metrics of best model on test data:\n",
    "        recall = recall_score(y_true_lab, y_pred_lab)\n",
    "        precision = precision_score(y_true_lab, y_pred_lab)\n",
    "        f1 = f1_score(y_true_lab, y_pred_lab)\n",
    "        confusion = confusion_matrix(y_true_lab, y_pred_lab)\n",
    "        TP = confusion[1, 1]\n",
    "        TN = confusion[0, 0]\n",
    "        FP = confusion[0, 1]\n",
    "        FN = confusion[1, 0]\n",
    "        specificity = TN / (TN + FP)\n",
    "        sensitivity = TP / (TP + FN)\n",
    "\n",
    "        fpr_num, tpr_num, _ = roc_curve(y_test_fold, y_pred_prob[:,1])\n",
    "        auc_score = auc(fpr_num, tpr_num)\n",
    "        \n",
    "        \n",
    "        #store metrics:\n",
    "        xg_model[MODEL]['best_model'].append(best_model)\n",
    "        xg_model[MODEL]['best_params'].append(best_model_params)\n",
    "        xg_model[MODEL]['feat_select'].append(feat_select)\n",
    "        xg_model[MODEL]['feat_rank'].append(feat_rank)\n",
    "        xg_model[MODEL]['feat_import'].append(feature_importance)\n",
    "        xg_model[MODEL]['true_lab'].append(y_true_lab)\n",
    "        xg_model[MODEL]['pred_lab'].append(y_pred_lab)\n",
    "        xg_model[MODEL]['pred_prob'].append(y_pred_prob)\n",
    "        xg_model[MODEL]['recall'].append(recall)\n",
    "        xg_model[MODEL]['precision'].append(precision)\n",
    "        xg_model[MODEL]['f1'].append(f1)\n",
    "        xg_model[MODEL]['specificity'].append(specificity)\n",
    "        xg_model[MODEL]['sensitivity'].append(sensitivity)\n",
    "        xg_model[MODEL]['best_mod_auc'].append(best_grid_model_score)\n",
    "        xg_model[MODEL]['best_mod_std'].append(best_grid_model_std)\n",
    "        xg_model[MODEL]['test_auc'].append(auc_score)\n",
    "        \n",
    "        print(xg_model[MODEL]) #allows you to idenitfy when a loop is complete and view that loops metrics \n",
    "        \n",
    "        %store xg_model #allows you to extract data later without having to re-run the whole loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9d0ae-db1c-4382-b3f3-8ffcab0c8c23",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c980d-975f-48d5-92ba-0e413318fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up model:\n",
    "model_names = [\"rf\"] #allows combination into a larger loop if able\n",
    "models = {'rf': RandomForestClassifier(random_state=23)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567541e-3b35-4bb8-b170-50ce0bfd2744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up parameter grid of different parameter values to try in the grid search:\n",
    "param_grids = {\n",
    "       'rf': {\n",
    "       'n_estimators' : [100, 200, 300, 400, 500],\n",
    "        'criterion' : ['gini', 'entropy'],\n",
    "        'max_depth' : [10, 20, 30, 40, 50],\n",
    "        'max_features' : [ 0.25, 0.5, 0.75, 1.0], \n",
    "        'max_samples': [0.8, 0.9, 1.0]\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1148350-a26a-4cf9-9795-47dbbcea5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create somewhere to store all the performance metrics:\n",
    "rf_model = {'rf':defaultdict(list)} #only run first time through as will over-ride results otherwise!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89abfd11-7add-4941-8da9-dbf4e6a04758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model for all folds of a 10 fold cross validation:\n",
    "#define the cross fold validation functions:\n",
    "skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 23) #for outer split \n",
    "skf_small = StratifiedKFold(n_splits=5, shuffle = True, random_state = 23) #for inner split within the grid search\n",
    "scorer = make_scorer(roc_auc_score, needs_threshold=True) #to use roc score in the grid search\n",
    "#outer loop which splits the data into 10 cross validation folds:\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    #define the training and testing data for the current fold:\n",
    "    x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "\n",
    "    #pre-process data (scale and normalise):\n",
    "    x_train_fold = pre_pros(x_train_fold)\n",
    "    x_test_fold = pre_pros(x_test_fold)\n",
    "    \n",
    "    #make y vector numerical:\n",
    "    y_train_fold = y_train_fold.replace({'IBD': 1, 'Control': 0})\n",
    "    y_test_fold = y_test_fold.replace({'IBD': 1, 'Control': 0})\n",
    "    \n",
    "    #perform grid search on the train data:\n",
    "    for i, MODEL in enumerate(model_names): #allows one loop for all models if processing allows\n",
    "        \n",
    "        time_start = time.time()\n",
    "        \n",
    "        model = models[MODEL] #select the corresponding model\n",
    "        param_grid = param_grids[MODEL] #select the corresponding parameter grid\n",
    "        grid = GridSearchCV(estimator = model, param_grid = param_grid , n_jobs = 1, cv = skf_small, scoring = scorer)\n",
    "        grid_result = grid.fit(x_train_fold, y_train_fold)\n",
    "       \n",
    "        time_grid = time.time() - time_start\n",
    "        \n",
    "        print(f\"Grid Search took {time_grid/60:.2f} mins ({time_grid:.2f} secs)\")\n",
    "    \n",
    "        #extract best model from the grid search:\n",
    "        best_model = grid_result.best_estimator_\n",
    "        best_model_params = grid_result.best_params_\n",
    "        best_idx = grid_result.best_index_\n",
    "        best_grid_model_score = grid_result.cv_results_['mean_test_score'][best_idx]\n",
    "        best_grid_model_std = grid_result.cv_results_['std_test_score'][best_idx]\n",
    "        \n",
    "        #Boruta feature selection on best model:\n",
    "        feat_selector = BorutaPy(best_model, n_estimators='auto', verbose=2, random_state=1)\n",
    "        x_train_fold_np  = np.asarray(x_train_fold)\n",
    "        y_train_fold_np  = np.asarray(y_train_fold)\n",
    "        feat_selector.fit(x_train_fold_np, y_train_fold_np)\n",
    "        \n",
    "        # extract selected features:\n",
    "        feat_select = feat_selector.support_\n",
    "        feat_rank = feat_selector.ranking_\n",
    "        \n",
    "        #fit new model with selected features and hyperparameters on all training data:\n",
    "        re_best_model = grid_result.best_estimator_ #new copy of best model\n",
    "        #extract only selected features:\n",
    "        x_train_red = x_train_fold.iloc[:, feat_select]\n",
    "        x_test_red = x_test_fold.iloc[:, feat_select]\n",
    "        re_best_model.fit(x_train_red, y_train_fold)\n",
    "    \n",
    "        #use best model with selected features on test data:\n",
    "        y_pred_lab = re_best_model.predict(x_test_red)\n",
    "        y_pred_prob = re_best_model.predict_proba(x_test_red)\n",
    "        y_true_lab = y_test_fold\n",
    "        feature_importance = re_best_model.feature_importances_\n",
    "        \n",
    "        #extract performance metrics of best model on test data:\n",
    "        recall = recall_score(y_true_lab, y_pred_lab)\n",
    "        precision = precision_score(y_true_lab, y_pred_lab)\n",
    "        f1 = f1_score(y_true_lab, y_pred_lab)\n",
    "        confusion = confusion_matrix(y_true_lab, y_pred_lab)\n",
    "        TP = confusion[1, 1]\n",
    "        TN = confusion[0, 0]\n",
    "        FP = confusion[0, 1]\n",
    "        FN = confusion[1, 0]\n",
    "        specificity = TN / (TN + FP)\n",
    "        sensitivity = TP / (TP + FN)\n",
    "\n",
    "        fpr_num, tpr_num, _ = roc_curve(y_test_fold, y_pred_prob[:,1])\n",
    "        auc_score = auc(fpr_num, tpr_num)\n",
    "        \n",
    "        \n",
    "        #store metrics:\n",
    "        rf_model[MODEL]['best_model'].append(best_model)\n",
    "        rf_model[MODEL]['best_params'].append(best_model_params)\n",
    "        rf_model[MODEL]['feat_select'].append(feat_select)\n",
    "        rf_model[MODEL]['feat_rank'].append(feat_rank)\n",
    "        rf_model[MODEL]['feat_import'].append(feature_importance)\n",
    "        rf_model[MODEL]['true_lab'].append(y_true_lab)\n",
    "        rf_model[MODEL]['pred_lab'].append(y_pred_lab)\n",
    "        rf_model[MODEL]['pred_prob'].append(y_pred_prob)\n",
    "        rf_model[MODEL]['recall'].append(recall)\n",
    "        rf_model[MODEL]['precision'].append(precision)\n",
    "        rf_model[MODEL]['f1'].append(f1)\n",
    "        rf_model[MODEL]['specificity'].append(specificity)\n",
    "        rf_model[MODEL]['sensitivity'].append(sensitivity)\n",
    "        rf_model[MODEL]['best_mod_auc'].append(best_grid_model_score)\n",
    "        rf_model[MODEL]['best_mod_std'].append(best_grid_model_std)\n",
    "        rf_model[MODEL]['test_auc'].append(auc_score)\n",
    "        \n",
    "        print(rf_model[MODEL]) #allows you to idenitfy when a loop is complete and view that loops metrics \n",
    "        \n",
    "        %store rf_model #allows you to extract data later without having to re-run the whole loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76792ef8-bb7b-438a-a86a-b259ac5905cd",
   "metadata": {},
   "source": [
    "## LASSO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25741ae6-06cc-4794-9d40-61175f9b36ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up model:\n",
    "model_names = [\"lasso\"]  #allows combination into a larger loop if able\n",
    "models = {'lasso': Lasso(random_state = 23) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d2577-f53c-4a2c-a220-d12d496ecf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up parameter grid of different parameter values to try in the grid search:\n",
    "param_grids = {\n",
    "       'lasso': {\n",
    "       'alpha' : list(np.logspace(-4,2,7))   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc260b9-620b-43eb-938b-011805b530d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create somewhere to store all the performance metrics:\n",
    "lasso_model = {'lasso':defaultdict(list)} #only run first time through as will over-ride results otherwise!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48e6e2-3ac5-49b0-9e17-9409947e7372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model for all folds of a 10 fold cross validation:\n",
    "#define the cross fold validation functions:\n",
    "skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 23) #for outer split \n",
    "skf_small = StratifiedKFold(n_splits=5, shuffle = True, random_state = 23) #for inner split within the grid search\n",
    "scorer = make_scorer(roc_auc_score, needs_threshold=True) #to use roc score in the grid search\n",
    "#outer loop which splits the data into 10 cross validation folds:\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    #define the training and testing data for the current fold:\n",
    "    x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "\n",
    "    #pre-process data (scale and normalise):\n",
    "    x_train_fold = pre_pros(x_train_fold)\n",
    "    x_test_fold = pre_pros(x_test_fold)\n",
    "    \n",
    "    #make y vector numerical:\n",
    "    y_train_fold = y_train_fold.replace({'IBD': 1, 'Control': 0})\n",
    "    y_test_fold = y_test_fold.replace({'IBD': 1, 'Control': 0})\n",
    "    \n",
    "    #perform grid search on the train data:\n",
    "    for i, MODEL in enumerate(model_names): #allows one loop for all models if processing allows\n",
    "        \n",
    "        time_start = time.time()\n",
    "        \n",
    "        model = models[MODEL] #select corresponding model\n",
    "        param_grid = param_grids[MODEL] #select corresponding parameter grid\n",
    "        grid = GridSearchCV(estimator = model, param_grid = param_grid , n_jobs = 1, cv = skf_small, scoring = scorer)\n",
    "        grid_result = grid.fit(x_train_fold, y_train_fold)\n",
    "       \n",
    "        time_grid = time.time() - time_start\n",
    "        \n",
    "        print(f\"Grid Search took {time_grid/60:.2f} mins ({time_grid:.2f} secs)\")\n",
    "    \n",
    "        #extract best model from the grid search:\n",
    "        best_model = grid_result.best_estimator_\n",
    "        best_model_params = grid_result.best_params_\n",
    "        best_idx = grid_result.best_index_\n",
    "        best_grid_model_score = grid_result.cv_results_['mean_test_score'][best_idx]\n",
    "        best_grid_model_std = grid_result.cv_results_['std_test_score'][best_idx]\n",
    "    \n",
    "        #extract important features as selected by the built in LASSO model:\n",
    "        feat_select = (best_model.coef_ != 0)\n",
    "    \n",
    "        #use best model parameters and features on test data:\n",
    "        y_pred_prob = best_model.predict(x_test_fold)\n",
    "        y_pred_lab = np.where(y_pred_prob >= 0.5, 1, 0) #defined as IBD if probability if over 0.5 \n",
    "        y_true_lab = y_test_fold\n",
    "        \n",
    "        #extract performance metrics of best model on test data:\n",
    "        recall = recall_score(y_true_lab, y_pred_lab)\n",
    "        precision = precision_score(y_true_lab, y_pred_lab)\n",
    "        f1 = f1_score(y_true_lab, y_pred_lab)\n",
    "        confusion = confusion_matrix(y_true_lab, y_pred_lab)\n",
    "        TP = confusion[1, 1]\n",
    "        TN = confusion[0, 0]\n",
    "        FP = confusion[0, 1]\n",
    "        FN = confusion[1, 0]\n",
    "        specificity = TN / (TN + FP)\n",
    "        sensitivity = TP / (TP + FN)\n",
    "\n",
    "        fpr_num, tpr_num, _ = roc_curve(y_test_fold, y_pred_prob)\n",
    "        auc_score = auc(fpr_num, tpr_num)\n",
    "        \n",
    "        \n",
    "        #store metrics:\n",
    "        lasso_model[MODEL]['best_model'].append(best_model)\n",
    "        lasso_model[MODEL]['best_params'].append(best_model_params)\n",
    "        lasso_model[MODEL]['true_lab'].append(y_true_lab)\n",
    "        lasso_model[MODEL]['pred_lab'].append(y_pred_lab)\n",
    "        lasso_model[MODEL]['pred_prob'].append(y_pred_prob)\n",
    "        lasso_model[MODEL]['recall'].append(recall)\n",
    "        lasso_model[MODEL]['precision'].append(precision)\n",
    "        lasso_model[MODEL]['feat_select'].append(feat_select)\n",
    "        lasso_model[MODEL]['f1'].append(f1)\n",
    "        lasso_model[MODEL]['specificity'].append(specificity)\n",
    "        lasso_model[MODEL]['sensitivity'].append(sensitivity)\n",
    "        lasso_model[MODEL]['best_mod_auc'].append(best_grid_model_score)\n",
    "        lasso_model[MODEL]['best_mod_std'].append(best_grid_model_std)\n",
    "        lasso_model[MODEL]['test_auc'].append(auc_score)\n",
    "        \n",
    "        print(lasso_model[MODEL]) #allows you to idenitfy when a loop is complete and view that loops metrics \n",
    "        \n",
    "        %store lasso_model  #allows you to extract data later without having to re-run the whole loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f13c366-ce7a-4888-80dc-e7f5399344f6",
   "metadata": {},
   "source": [
    "# Extract feature names and visualize model performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896d7fd-3e12-48c1-931f-595a556c9e76",
   "metadata": {},
   "source": [
    "### Extract model performance metrics from store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a3498f-8bb2-4bf1-aafc-433b5a76ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "store -r lasso_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638f60f2-7140-4d84-8705-1aba92bb9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "store -r rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "543ee89c-f1d6-497e-b07f-19c9156fb30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "store -r xg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedbbe42-3da8-4c43-a1f6-01133f7d364e",
   "metadata": {},
   "source": [
    "### Extract feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580a9c9-cc03-4015-88a7-13333e389132",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_features = []\n",
    "#append features names to a list from the data for all folds:\n",
    "for fold in range(0,10):\n",
    "    imp_feat = X.iloc[:,(lasso_model['lasso']['feat_select'][fold] == True)]\n",
    "    features = imp_feat.columns\n",
    "    \n",
    "    lasso_features.append(list(features))\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd42a53-e5fe-4673-91bc-726ef6213471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all features from all folds into one big list:\n",
    "lasso_features = list(chain(*lasso_features))\n",
    "#extract the names of all the unique features:\n",
    "unique_lasso = np.unique(lasso_features)\n",
    "#find the number of unique features:\n",
    "len(unique_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdeb674-5d92-4136-902e-4af6e817f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_features = []\n",
    "#append features names to a list from the data for all folds:\n",
    "for fold in range(0,10):\n",
    "    imp_feat = X.iloc[:,(rf_model['rf']['feat_select'][fold] == True)]\n",
    "    features = imp_feat.columns\n",
    "    \n",
    "    rf_features.append(list(features))\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e255ae-fd84-4d13-ac8c-10421a09cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all features from all folds into one big list:\n",
    "rf_features = list(chain(*rf_features))\n",
    "#extract the names of all the unique features:\n",
    "unique_rf = np.unique(rf_features)\n",
    "#find the number of unique features:\n",
    "len(unique_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2766c-0ab1-47c0-8dc7-a96e1da1a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_features = []\n",
    "#append feature names to a list from the data for all folds:\n",
    "for fold in range(0,10):\n",
    "    imp_feat = X.iloc[:,(xg_model['xgboost']['feat_select'][fold] == True)]\n",
    "    features = imp_feat.columns\n",
    "    \n",
    "    xg_features.append(list(features))\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ff6b3-e158-44df-b5bd-2ea6b035c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all features from all folds into one big list:\n",
    "xg_features = list(chain(*xg_features))\n",
    "#extract the names of all the unique features:\n",
    "unique_xg = np.unique(xg_features)\n",
    "#find the number of unique features:\n",
    "len(unique_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8334f7-4890-490a-96e9-82fa975385c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain the number of features that are identified by more than one model:\n",
    "len(set(unique_xg) & set(unique_rf) & set(unique_lasso))\n",
    "len((set(unique_rf) & set(unique_xg)))\n",
    "len((set(unique_xg) & set(unique_lasso)))\n",
    "len((set(unique_rf) & set(unique_lasso)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ee54f-8413-4b45-b3c9-c4b51fac2b7d",
   "metadata": {},
   "source": [
    "### Extract the average and sd of the performance metrics across the 10 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e025dae-b51e-43b3-aac4-2e107b458822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost averages:\n",
    "print(np.mean(xg_model['xgboost']['recall']))\n",
    "print(np.mean(xg_model['xgboost']['precision']))\n",
    "print(np.mean(xg_model['xgboost']['f1']))\n",
    "print(np.mean(xg_model['xgboost']['specificity']))\n",
    "print(np.mean(xg_model['xgboost']['sensitivity']))\n",
    "print(np.mean(xg_model['xgboost']['test_auc']))\n",
    "#xgboost sds:\n",
    "print(np.std(xg_model['xgboost']['recall']))\n",
    "print(np.std(xg_model['xgboost']['precision']))\n",
    "print(np.std(xg_model['xgboost']['f1']))\n",
    "print(np.std(xg_model['xgboost']['specificity']))\n",
    "print(np.std(xg_model['xgboost']['sensitivity']))\n",
    "print(np.std(xg_model['xgboost']['test_auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8e6622-b033-4a36-9da6-b09de2a8418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest averages:\n",
    "print(np.mean(rf_model['rf']['recall']))\n",
    "print(np.mean(rf_model['rf']['precision']))\n",
    "print(np.mean(rf_model['rf']['f1']))\n",
    "print(np.mean(rf_model['rf']['specificity']))\n",
    "print(np.mean(rf_model['rf']['sensitivity']))\n",
    "print(np.mean(rf_model['rf']['test_auc']))\n",
    "#random forest sds:\n",
    "print(np.std(rf_model['rf']['recall']))\n",
    "print(np.std(rf_model['rf']['precision']))\n",
    "print(np.std(rf_model['rf']['f1']))\n",
    "print(np.std(rf_model['rf']['specificity']))\n",
    "print(np.std(rf_model['rf']['sensitivity']))\n",
    "print(np.std(rf_model['rf']['test_auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca23ce-aeb2-4d4e-a0d1-b6f84e7d0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso averages:\n",
    "print(np.mean(lasso_model['lasso']['recall']))\n",
    "print(np.mean(lasso_model['lasso']['precision']))\n",
    "print(np.mean(lasso_model['lasso']['f1']))\n",
    "print(np.mean(lasso_model['lasso']['specificity']))\n",
    "print(np.mean(lasso_model['lasso']['sensitivity']))\n",
    "print(np.mean(lasso_model['lasso']['test_auc']))\n",
    "#lasso sds:\n",
    "print(np.std(lasso_model['lasso']['recall']))\n",
    "print(np.std(lasso_model['lasso']['precision']))\n",
    "print(np.std(lasso_model['lasso']['f1']))\n",
    "print(np.std(lasso_model['lasso']['specificity']))\n",
    "print(np.std(lasso_model['lasso']['sensitivity']))\n",
    "print(np.std(lasso_model['lasso']['test_auc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d0f75-c7a6-45a6-9884-568f9dcbc721",
   "metadata": {},
   "source": [
    "### ROC curve of all the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb4441-445d-4b08-a170-d662755bf5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists to store true positive rates and set false positive rates:\n",
    "tprs_rf = []\n",
    "base_fpr_rf = np.linspace(0, 1, 101)\n",
    "tprs_xg = []\n",
    "base_fpr_xg = np.linspace(0, 1, 101)\n",
    "tprs_lasso = []\n",
    "base_fpr_lasso = np.linspace(0, 1, 101)\n",
    "\n",
    "#set up blank figure space:\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "\n",
    "#calculate the corresponding tprs for the fprs of each fold of each model:\n",
    "for i in range(0,10):\n",
    "    fpr, tpr, _ = roc_curve(rf_model['rf']['true_lab'][i], rf_model['rf']['pred_prob'][i][:,1])\n",
    "    \n",
    "    tpr = np.interp(base_fpr_rf, fpr, tpr)\n",
    "    tpr[0] = 0.0\n",
    "    tprs_rf.append(tpr)\n",
    "\n",
    "for i in range(0,10):\n",
    "    fpr, tpr, _ = roc_curve(lasso_model['lasso']['true_lab'][i], lasso_model['lasso']['pred_prob'][i])\n",
    "    \n",
    "    tpr = np.interp(base_fpr_lasso, fpr, tpr)\n",
    "    tpr[0] = 0.0\n",
    "    tprs_lasso.append(tpr)\n",
    "\n",
    "for i in range(0,10):\n",
    "    fpr, tpr, _ = roc_curve(xg_model['xgboost']['true_lab'][i], xg_model['xgboost']['pred_prob'][i][:,1])\n",
    "    \n",
    "    tpr = np.interp(base_fpr_xg, fpr, tpr)\n",
    "    tpr[0] = 0.0\n",
    "    tprs_xg.append(tpr)\n",
    "    \n",
    "#find the average and sd of the tprs for each fold of each model:\n",
    "tprs_rf = np.array(tprs_rf)\n",
    "mean_tprs_rf = tprs_rf.mean(axis=0)\n",
    "std_rf = tprs_rf.std(axis=0)\n",
    "\n",
    "tprs_lasso = np.array(tprs_lasso)\n",
    "mean_tprs_lasso = tprs_lasso.mean(axis=0)\n",
    "std_lasso = tprs_lasso.std(axis=0)\n",
    "\n",
    "tprs_xg = np.array(tprs_xg)\n",
    "mean_tprs_xg = tprs_xg.mean(axis=0)\n",
    "std_xg = tprs_xg.std(axis=0)\n",
    "\n",
    "#find the confidence interval for the average tprs of each model:\n",
    "tprs_upper_rf = np.minimum(mean_tprs_rf + std_rf, 1)\n",
    "tprs_lower_rf = mean_tprs_rf - std_rf\n",
    "tprs_upper_xg = np.minimum(mean_tprs_xg + std_xg, 1)\n",
    "tprs_lower_xg = mean_tprs_xg - std_xg\n",
    "tprs_upper_lasso = np.minimum(mean_tprs_lasso + std_lasso, 1)\n",
    "tprs_lower_lasso = mean_tprs_lasso - std_lasso\n",
    "\n",
    "#calculate the area under the roc for each of the average models performance:\n",
    "auc_rf = auc(base_fpr_rf, mean_tprs_rf)\n",
    "auc_xg = auc(base_fpr_xg, mean_tprs_xg)\n",
    "auc_lasso = auc(base_fpr_lasso, mean_tprs_lasso)\n",
    "\n",
    "#plot the average roc for each model with the confidence interval surrounding:\n",
    "plt.plot(base_fpr_rf, mean_tprs_rf, 'palevioletred', label=r'Mean ROC RandomForest (AUC = {} $\\pm$ {})'.format(round(auc_rf,2), 0.04))\n",
    "plt.fill_between(base_fpr_rf, tprs_lower_rf, tprs_upper_rf, color='pink', alpha=0.3)\n",
    "plt.plot(base_fpr_xg, mean_tprs_xg, 'darkcyan', label=r'Mean ROC XGBoost (AUC = {} $\\pm$ {})'.format(round(auc_xg,2), 0.05))\n",
    "plt.fill_between(base_fpr_xg, tprs_lower_xg, tprs_upper_xg, color='deepskyblue', alpha=0.3)\n",
    "plt.plot(base_fpr_lasso, mean_tprs_lasso, 'orange', label=r'Mean ROC LASSO (AUC = {} $\\pm$ {})'.format(round(auc_lasso,2), 0.04))\n",
    "plt.fill_between(base_fpr_lasso, tprs_lower_lasso, tprs_upper_lasso, color='sandybrown', alpha=0.2)\n",
    "\n",
    "#plot on the blank figure space and add title labels:\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('Receiver Operating Characteristic Microbiome Model Averages')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.savefig('micro_avg.pdf', format = 'pdf', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 [python/3.9.6]",
   "language": "python",
   "name": "sys_python396"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
